{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae72ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "# Load English dictionary\n",
    "english_dictionary = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5fa3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "from tkinter import *\n",
    "import tkinter.font as font\n",
    "from nltk import ngrams\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import wordninja\n",
    "import pickle\n",
    "from nltk import edit_distance\n",
    "# import enchant\n",
    "import eng_to_ipa as eng_to_ipa\n",
    "from nltk.util import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a325c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved unigram dictionary from the file\n",
    "with open(\"unique_unigram_list.pickle\", \"rb\") as file:\n",
    "    unigram_list = pickle.load(file)\n",
    "    \n",
    "with open(\"bigram_prob_dict_final.pickle\", \"rb\") as file:\n",
    "    bigram_probabilities = pickle.load(file)\n",
    "bigram_probabilities[('</s>', '<s>')] = 1\n",
    "    \n",
    "with open(\"unigram_ipa.pickle\", \"rb\") as file:\n",
    "    unigrams_ipa = pickle.load(file)\n",
    "\n",
    "#Load the Counter object from the file\n",
    "with open('unigrams_counter_final.pickle', 'rb') as file:\n",
    "    words_counter = pickle.load(file)\n",
    "\n",
    "\n",
    "unigrams_ipa_dict = {entry['token']: entry.get('ipa') for entry in unigrams_ipa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a8a6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'astrology'), ('astrology', 'has')] - [0.0058]\n",
      "[('<s>', 'astrology'), ('astrology', 'has')] - [0.0058, 0.00997]\n",
      "[('astrology', 'has'), ('has', 'been')] - [0.00997]\n",
      "[('astrology', 'has'), ('has', 'been')] - [0.00997, 0.09444]\n",
      "[('has', 'been'), ('been', 'describe')] - [0.09444]\n",
      "[('has', 'been'), ('been', 'describe')] - [0.09444, 0.0]\n",
      "[('been', 'describe'), ('describe', 'as')] - [0.0]\n",
      "[('been', 'describe'), ('describe', 'as')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('describe', 'as'), ('as', 'a')] - [0.0]\n",
      "[('describe', 'as'), ('as', 'a')] - [0.0, 0.19016]\n",
      "[('as', 'a'), ('a', 'stairway')] - [0.19016]\n",
      "[('as', 'a'), ('a', 'stairway')] - [0.19016, 0.0]\n",
      "[('stairway', 'leading'), ('leading', 'into')] - [0.0]\n",
      "[('stairway', 'leading'), ('leading', 'into')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('leading', 'into'), ('into', 'your')] - [0.0]\n",
      "[('leading', 'into'), ('into', 'your')] - [0.0, 0.05574]\n",
      "[('into', 'your'), ('your', 'deeper')] - [0.05574]\n",
      "[('into', 'your'), ('your', 'deeper')] - [0.05574, 0.00061]\n",
      "[('your', 'deeper'), ('deeper', 'self')] - [0.00061]\n",
      "[('your', 'deeper'), ('deeper', 'self')] - [0.00061, 0.0]\n",
      "[('deeper', 'self'), ('self', '</s>')] - [0.0]\n",
      "[('deeper', 'self'), ('self', '</s>')] - [0.0, 0.05217]\n",
      "[('<s>', 'it'), ('it', 'holds')] - [0.0104]\n",
      "[('<s>', 'it'), ('it', 'holds')] - [0.0104, 0.00186]\n",
      "[('it', 'holds'), ('holds', 'out')] - [0.00186]\n",
      "[('it', 'holds'), ('holds', 'out')] - [0.00186, 0.1]\n",
      "[('holds', 'out'), ('out', 'the')] - [0.1]\n",
      "[('holds', 'out'), ('out', 'the')] - [0.1, 0.07306]\n",
      "[('out', 'the'), ('the', 'promisses')] - [0.07306]\n",
      "[('out', 'the'), ('the', 'promisses')] - [0.07306, 0.0]\n",
      "[('promisses', 'that'), ('that', 'you')] - [0.0]\n",
      "[('promisses', 'that'), ('that', 'you')] - [0.0, 0.09625]\n",
      "[('that', 'you'), ('you', 'do')] - [0.09625]\n",
      "[('that', 'you'), ('you', 'do')] - [0.09625, 0.01351]\n",
      "[('you', 'do'), ('do', 'not')] - [0.01351]\n",
      "[('you', 'do'), ('do', 'not')] - [0.01351, 0.17021]\n",
      "[('do', 'not'), ('not', 'have')] - [0.17021]\n",
      "[('do', 'not'), ('not', 'have')] - [0.17021, 0.02037]\n",
      "[('not', 'have'), ('have', 'to')] - [0.02037]\n",
      "[('not', 'have'), ('have', 'to')] - [0.02037, 0.04275]\n",
      "[('have', 'to'), ('to', 'pass')] - [0.04275]\n",
      "[('have', 'to'), ('to', 'pass')] - [0.04275, 0.0]\n",
      "[('to', 'pass'), ('pass', 'through')] - [0.0]\n",
      "[('to', 'pass'), ('pass', 'through')] - [0.0, 0.5]\n",
      "[('pass', 'through'), ('through', 'life')] - [0.5]\n",
      "[('pass', 'through'), ('through', 'life')] - [0.5, 0.00685]\n",
      "[('through', 'life'), ('life', 'reacting')] - [0.00685]\n",
      "[('through', 'life'), ('life', 'reacting')] - [0.00685, 0.0]\n",
      "[('reacting', 'blindly'), ('blindly', 'too')] - [0.0]\n",
      "[('reacting', 'blindly'), ('blindly', 'too')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('blindly', 'too'), ('too', 'experience')] - [0.0]\n",
      "[('blindly', 'too'), ('too', 'experience')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('too', 'experience'), ('experience', 'that')] - [0.0]\n",
      "[('too', 'experience'), ('experience', 'that')] - [0.0, 0.07407]\n",
      "[('experience', 'that'), ('that', 'you')] - [0.07407]\n",
      "[('experience', 'that'), ('that', 'you')] - [0.07407, 0.09625]\n",
      "[('that', 'you'), ('you', 'can')] - [0.09625]\n",
      "[('that', 'you'), ('you', 'can')] - [0.09625, 0.0409]\n",
      "[('you', 'can'), ('can', 'within')] - [0.0409]\n",
      "[('you', 'can'), ('can', 'within')] - [0.0409, 0.0]\n",
      "[('can', 'within'), ('within', 'limits')] - [0.0]\n",
      "[('can', 'within'), ('within', 'limits')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('within', 'limits'), ('limits', 'direct')] - [0.0]\n",
      "[('within', 'limits'), ('limits', 'direct')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('limits', 'direct'), ('direct', 'your')] - [0.0]\n",
      "[('limits', 'direct'), ('direct', 'your')] - [0.0, 0.06667]\n",
      "[('direct', 'your'), ('your', 'own')] - [0.06667]\n",
      "[('direct', 'your'), ('your', 'own')] - [0.06667, 0.04179]\n",
      "[('your', 'own'), ('own', 'destiny')] - [0.04179]\n",
      "[('your', 'own'), ('own', 'destiny')] - [0.04179, 0.0]\n",
      "[('own', 'destiny'), ('destiny', 'and')] - [0.0]\n",
      "[('own', 'destiny'), ('destiny', 'and')] - [0.0, 0.2]\n",
      "[('destiny', 'and'), ('and', 'in')] - [0.2]\n",
      "[('destiny', 'and'), ('and', 'in')] - [0.2, 0.00475]\n",
      "[('and', 'in'), ('in', 'the')] - [0.00475]\n",
      "[('and', 'in'), ('in', 'the')] - [0.00475, 0.1513]\n",
      "[('in', 'the'), ('the', 'pocess')] - [0.1513]\n",
      "[('in', 'the'), ('the', 'pocess')] - [0.1513, 0.0]\n",
      "[('pocess', 'rich'), ('rich', 'a')] - [0.0]\n",
      "[('pocess', 'rich'), ('rich', 'a')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('rich', 'a'), ('a', 'true')] - [0.0]\n",
      "[('rich', 'a'), ('a', 'true')] - [0.0, 0.00194]\n",
      "[('a', 'true'), ('true', 'self')] - [0.00194]\n",
      "[('a', 'true'), ('true', 'self')] - [0.00194, 0.0]\n",
      "[('true', 'self'), ('self', 'understanding')] - [0.0]\n",
      "[('true', 'self'), ('self', 'understanding')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('self', 'understanding'), ('understanding', '</s>')] - [0.0]\n",
      "[('self', 'understanding'), ('understanding', '</s>')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('<s>', 'one'), ('one', 'surprising')] - [0.00725]\n",
      "[('<s>', 'one'), ('one', 'surprising')] - [0.00725, 0.0]\n",
      "[('one', 'surprising'), ('surprising', 'thing')] - [0.0]\n",
      "[('one', 'surprising'), ('surprising', 'thing')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('surprising', 'thing'), ('thing', 'about')] - [0.0]\n",
      "[('surprising', 'thing'), ('thing', 'about')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('thing', 'about'), ('about', 'the')] - [0.0]\n",
      "[('thing', 'about'), ('about', 'the')] - [0.0, 0.1085]\n",
      "[('about', 'the'), ('the', 'sciene')] - [0.1085]\n",
      "[('about', 'the'), ('the', 'sciene')] - [0.1085, 0.0]\n",
      "[('sciene', 'of'), ('of', 'the')] - [0.0]\n",
      "[('sciene', 'of'), ('of', 'the')] - [0.0, 0.18447]\n",
      "[('of', 'the'), ('the', 'stars')] - [0.18447]\n",
      "[('of', 'the'), ('the', 'stars')] - [0.18447, 0.00186]\n",
      "[('the', 'stars'), ('stars', 'is')] - [0.00186]\n",
      "[('the', 'stars'), ('stars', 'is')] - [0.00186, 0.0]\n",
      "[('stars', 'is'), ('is', 'its')] - [0.0]\n",
      "[('stars', 'is'), ('is', 'its')] - [0.0, 0.0007]\n",
      "[('is', 'its'), ('its', 'constant')] - [0.0007]\n",
      "[('is', 'its'), ('its', 'constant')] - [0.0007, 0.0]\n",
      "[('its', 'constant'), ('constant', 'newness')] - [0.0]\n",
      "[('its', 'constant'), ('constant', 'newness')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('<s>', 'down'), ('down', 'through')] - [0.0]\n",
      "[('<s>', 'down'), ('down', 'through')] - [0.0, 0.02778]\n",
      "[('down', 'through'), ('through', 'the')] - [0.02778]\n",
      "[('down', 'through'), ('through', 'the')] - [0.02778, 0.16781]\n",
      "[('through', 'the'), ('the', 'centuries')] - [0.16781]\n",
      "[('through', 'the'), ('the', 'centuries')] - [0.16781, 0.0]\n",
      "[('the', 'centuries'), ('centuries', 'philosofers')] - [0.0]\n",
      "[('the', 'centuries'), ('centuries', 'philosofers')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('mystics', 'and'), ('and', 'spirytual')] - [0.0]\n",
      "[('mystics', 'and'), ('and', 'spirytual')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('spirytual', 'teachers'), ('teachers', 'have')] - [0.0]\n",
      "[('spirytual', 'teachers'), ('teachers', 'have')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('teachers', 'have'), ('have', 'studied')] - [0.0]\n",
      "[('teachers', 'have'), ('have', 'studied')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('have', 'studied'), ('studied', 'writen')] - [0.0]\n",
      "[('have', 'studied'), ('studied', 'writen')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('writen', 'about'), ('about', 'refined')] - [0.0]\n",
      "[('writen', 'about'), ('about', 'refined')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('refined', 'and'), ('and', 'pondered')] - [0.16667]\n",
      "[('refined', 'and'), ('and', 'pondered')] - [0.16667, 0.0]\n",
      "[('<s>', 'it'), ('it', 'is')] - [0.0104]\n",
      "[('<s>', 'it'), ('it', 'is')] - [0.0104, 0.17658]\n",
      "[('it', 'is'), ('is', 'the')] - [0.17658]\n",
      "[('it', 'is'), ('is', 'the')] - [0.17658, 0.11634]\n",
      "[('is', 'the'), ('the', 'worlds')] - [0.11634]\n",
      "[('is', 'the'), ('the', 'worlds')] - [0.11634, 0.00152]\n",
      "[('the', 'worlds'), ('worlds', 'oldest')] - [0.00152]\n",
      "[('the', 'worlds'), ('worlds', 'oldest')] - [0.00152, 0.0]\n",
      "[('worlds', 'oldest'), ('oldest', 'science')] - [0.0]\n",
      "[('worlds', 'oldest'), ('oldest', 'science')] - [0.0, 1.0]\n",
      "[('oldest', 'science'), ('science', 'and')] - [1.0]\n",
      "[('oldest', 'science'), ('science', 'and')] - [1.0, 0.2381]\n",
      "[('science', 'and'), ('and', 'our')] - [0.2381]\n",
      "[('science', 'and'), ('and', 'our')] - [0.2381, 0.00079]\n",
      "[('and', 'our'), ('our', 'own')] - [0.00079]\n",
      "[('and', 'our'), ('our', 'own')] - [0.00079, 0.02256]\n",
      "[('our', 'own'), ('own', 'generations')] - [0.02256]\n",
      "[('our', 'own'), ('own', 'generations')] - [0.02256, 0.0]\n",
      "[('newest', 'subject'), ('subject', 'off')] - [0.0]\n",
      "[('newest', 'subject'), ('subject', 'off')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('subject', 'off'), ('off', 'inquiry')] - [0.0]\n",
      "[('subject', 'off'), ('off', 'inquiry')] - [0.0, 0.0]\n",
      "WRONG\n",
      "[('off', 'inquiry'), ('inquiry', '</s>')] - [0.0]\n",
      "[('off', 'inquiry'), ('inquiry', '</s>')] - [0.0, 0.0]\n",
      "WRONG\n"
     ]
    }
   ],
   "source": [
    "clicked_word = \"\"\n",
    "\n",
    "# re patterns\n",
    "#remove url\n",
    "url_pattern_1 =r'(http[s]?://)?[a-zA-Z0-9]+([-.][a-zA-Z0-9]+)\\.[a-zA-Z]{2,3}(/\\S)?'\n",
    "url_pattern_2 = r'http\\S+|www.\\S+'\n",
    "\n",
    "#remove '-\\n' patterns\n",
    "hyphen_nl_pattern = r'(-\\n)'\n",
    "\n",
    "#remove hyphens that are found in between a word\n",
    "hyphen_pattern = r'(?<=[a-z])-(?=[a-z])'\n",
    "\n",
    "#remove digits\n",
    "digits = '\\d+'\n",
    "\n",
    "#remove \\n\n",
    "next_line = r'\\n'\n",
    "\n",
    "#remove any symbols\n",
    "symbols = r'[^\\w\\s]'\n",
    "\n",
    "#remove double spaces\n",
    "double_space = r'\\s{2}' \n",
    "\n",
    "\n",
    "def sent_preprocess(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences_clean = []\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = re.sub(url_pattern_1, '', sentence)\n",
    "        cleaned_sentence = re.sub(url_pattern_2, '', cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(hyphen_nl_pattern, '', cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(hyphen_pattern, ' ', cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(next_line,' ', cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(digits,'', cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(symbols,'', cleaned_sentence)\n",
    "        cleaned_sentence = re.sub(double_space,' ', cleaned_sentence)\n",
    "        cleaned_sentence = cleaned_sentence.lower()\n",
    "        sentences_clean.append(cleaned_sentence)\n",
    "    return sentences_clean\n",
    "\n",
    "def unnest_list(nested_list):\n",
    "    \"\"\"Unnest a list to convert nestted token in list form into unnested one\n",
    "    \n",
    "    nested_list = [1, [2, 3], [4, [5, 6]], 7, [8, [9, 10]]]\n",
    "    \n",
    "    Example:\n",
    "\n",
    "    flat_list = unnest_list(nested_list)\n",
    "    print(flat_list)\n",
    "\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\"\"\"\n",
    "    \n",
    "    unnested = []\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            unnested.extend(unnest_list(item))\n",
    "        else:\n",
    "            unnested.append(item)\n",
    "    return unnested\n",
    "\n",
    "def break_down(texts):\n",
    "    \"\"\"Start from preprocess the texts by removing noises such as punctuation and lower case it. Then tokenize the texts into\n",
    "    individual words and return it in unnested list\"\"\"\n",
    "    sentences = sent_preprocess(texts)\n",
    "    \n",
    "    token = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        padded = list(pad_sequence(words, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=2))\n",
    "        token.append(padded)\n",
    "    \n",
    "    flat_token = unnest_list(token)\n",
    "    \n",
    "    return(flat_token)\n",
    "\n",
    "# function to obtain ipa with a word\n",
    "def get_ipa(word):\n",
    "    return unigrams_ipa_dict.get(word)\n",
    "\n",
    "#generate probability of word occuring in the corpus\n",
    "def P(word, N=sum(words_counter.values())): \n",
    "    # Returns probability of a word in vocabulary.\n",
    "    return words_counter[word] / N\n",
    "\n",
    "def initialize_dictionary_list():\n",
    "    # Clear the previous content in the dictionary_list widget\n",
    "    dictionary_list.delete(0, \"end\")\n",
    "\n",
    "    # Display the dictionary list in the dictionary_list widget\n",
    "    for word in unigram_list:\n",
    "        word_entry = f\"{word}\\n\"\n",
    "        dictionary_list.insert(END, word_entry)\n",
    "    \n",
    "def combine_funcs(*funcs):\n",
    "    \"\"\"to combine multiple function inside one command when click on the button\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    combined_func(print(), list(), max())\"\"\"\n",
    "    \n",
    "    def combined_func(*args, **kwargs):\n",
    "        for f in funcs:\n",
    "            f(*args, **kwargs)\n",
    "    return combined_func\n",
    "    \n",
    "def detect_non_word_errors():\n",
    "    \"\"\"To detect non-word error based on dictionary look up and highlight the text\"\"\"\n",
    "    # Get the content of the text widget\n",
    "    input_text = editor.get(\"1.0\", \"end\")\n",
    "    \n",
    "    # set the dictionary of known words\n",
    "    known_words = unigram_list\n",
    "    \n",
    "    # split the input text and preprocess it\n",
    "    words = break_down(input_text)\n",
    "    \n",
    "    # check if the word is a non-word error\n",
    "    for word in words:\n",
    "        if word in known_words:\n",
    "            continue\n",
    "        elif word in english_dictionary:\n",
    "            continue\n",
    "        else:\n",
    "            start_index = editor.search(word, \"1.0\", \"end\", exact=True, nocase=True)\n",
    "            if start_index:\n",
    "                end_index = f\"{start_index}+{len(word)}c\"\n",
    "                editor.tag_add(\"non-word error\", start_index, end_index)\n",
    "            \n",
    "def calculate_bigram_probability(bigram):\n",
    "    return bigram_probabilities.get(bigram, 0.0)\n",
    "\n",
    "def detect_real_word_errors():\n",
    "\n",
    "    # Get the content of the text widget\n",
    "    input_text = editor.get(\"1.0\", \"end\")\n",
    "    # set the dictionary of known words\n",
    "    # known_words = unigram_list\n",
    "    # split the input text and preprocess it\n",
    "    window_size = 2\n",
    "    min_bigram_probability = 0.00001\n",
    "    words = break_down(input_text)\n",
    "    #print(words)\n",
    "\n",
    "    for i in range(1, len(words)):\n",
    "        current_word = words[i]\n",
    "        \n",
    "        # Check bigram probability\n",
    "        if i > 0:\n",
    "            if current_word not in unigram_list:\n",
    "                continue\n",
    "            previous_word = words[i-1]\n",
    "            if i < len(words)-1:\n",
    "                next_word = words[i+1]\n",
    "            else: pass\n",
    "            bigram = list(ngrams([previous_word, current_word,next_word], window_size))\n",
    "            bigram_probabilities = []\n",
    "            for bg in bigram:\n",
    "                bigram_probabilities.append(calculate_bigram_probability(bg))\n",
    "                print(bigram,'-',bigram_probabilities)\n",
    "            avg_probability = np.mean(bigram_probabilities)\n",
    "            #print(avg_probability)\n",
    "            if avg_probability >= min_bigram_probability:\n",
    "                #print('skipped')\n",
    "                continue\n",
    "            else: print('WRONG')\n",
    "        #Find the start and end index of the word\n",
    "        start_index = editor.search(current_word, \"1.0\", \"end\", exact = True, nocase = True)\n",
    "        #curr_word_pattern = r\"\\b\" + re.escape(current_word) + r\"\\b\"\n",
    "        #start_index = editor.search(curr_word_pattern, \"1.0\", \"end\", regexp=True, exact = False)\n",
    "        #print(start_index)\n",
    "        while start_index:\n",
    "            end_index = f\"{start_index}+{len(current_word)}c\"\n",
    "        #print(end_index)\n",
    "\n",
    "        # Add a tag to highlight the real word error\n",
    "            editor.tag_add(\"real word error\", start_index, end_index)\n",
    "            start_index = editor.search(current_word, end_index, \"end\", exact=True, nocase=True)\n",
    "            \n",
    "def generate_word_candidates(event):\n",
    "    global clicked_word\n",
    "    \n",
    "    # Get the index of the clicked word\n",
    "    index = editor.index(\"@%s,%s wordstart\" % (event.x, event.y))\n",
    "    \n",
    "    # Get the word at the clicked index\n",
    "    clicked_word = editor.get(index + \" wordstart\", index + \" wordend\").lower()\n",
    "    \n",
    "    # Define corpus and bigram_probabilities\n",
    "    corpus = unigram_list\n",
    "    global bigram_probabilities\n",
    "    \n",
    "    # Find the index of the mispelled word in the sentence \n",
    "    sentence = editor.get(\"1.0\", \"end-1c\")\n",
    "    words = break_down(sentence)\n",
    "    misspelled_index = words.index(clicked_word)\n",
    "    \n",
    "    candidates = []\n",
    "    # Generate word candidates using edit distance and bigram probability \n",
    "    for word in corpus:\n",
    "        if len(word) <= 7:\n",
    "            if edit_distance(word, clicked_word, transpositions=True) <= 1:\n",
    "                candidates.append(word)\n",
    "        else:\n",
    "            if edit_distance(word, clicked_word, transpositions=True) <= 2:\n",
    "                candidates.append(word)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for candidate in candidates:\n",
    "        # Get previous word for bigram probabilities computation\n",
    "        previous_word = words[misspelled_index - 1]\n",
    "        bigram_prob = calculate_bigram_probability((previous_word,candidate))\n",
    "        probabilities[candidate] = bigram_prob\n",
    "    \n",
    "    sorted_dict = {k: v for k, v in sorted(probabilities.items(), key=lambda x: x[1],reverse=True)}\n",
    "    probabilities_list = list(sorted_dict.keys())\n",
    "    \n",
    "    \n",
    "    # Clear the existing text in the correction label widget\n",
    "    suggested_candidates.delete(0, \"end\")\n",
    "\n",
    "    # Generate the word candidates in the correction label widget\n",
    "    for candidate in probabilities_list:\n",
    "        suggested_candidates.insert(\"end\", candidate + '\\n')\n",
    "\n",
    "def generate_real_word_candidates(event):\n",
    "    global clicked_word\n",
    "    \n",
    "    # Get the index of the clicked word\n",
    "    index = editor.index(\"@%s,%s wordstart\" % (event.x, event.y))\n",
    "    \n",
    "    # Get the word at the clicked index\n",
    "    clicked_word = editor.get(index + \" wordstart\", index + \" wordend\").lower()\n",
    "    clicked_word_lower = clicked_word.lower()\n",
    "    \n",
    "    # Define corpus and bigram_probabilities\n",
    "    corpus = unigram_list\n",
    "    global bigram_probabilities\n",
    "    \n",
    "    # Find the index of the mispelled word in the sentence \n",
    "    sentence = editor.get(\"1.0\", \"end-1c\")\n",
    "    words = break_down(sentence)\n",
    "    misspelled_index = words.index(clicked_word_lower)\n",
    "    \n",
    "    candidates = []\n",
    "    word_ipa = eng_to_ipa.convert(eng_to_ipa.convert(clicked_word_lower))\n",
    "    for word in corpus:\n",
    "        ed = edit_distance(word, clicked_word_lower, transpositions=True)\n",
    "        if len(word) <= 7:\n",
    "            if ed <= 1:\n",
    "                candidates.append(word)\n",
    "            if ed <=4:\n",
    "                w_ipa = get_ipa(word)\n",
    "                if w_ipa is not None:\n",
    "                    ed_ipa = edit_distance(w_ipa, word_ipa)\n",
    "                    if ed_ipa ==0:\n",
    "                        candidates.append(word)    \n",
    "        else:\n",
    "            if ed <= 2:\n",
    "                candidates.append(word)\n",
    "                            \n",
    "    candidates = list(set(candidates))\n",
    "    \n",
    "    probabilities = {}\n",
    "    for candidate in candidates:\n",
    "        # Get previous word for bigram probabilities computation\n",
    "        previous_word = words[misspelled_index - 1]\n",
    "        next_word = words[misspelled_index + 1]\n",
    "        prob = P(candidate)*calculate_bigram_probability((previous_word,candidate))*calculate_bigram_probability((candidate,next_word))\n",
    "        probabilities[candidate] = prob\n",
    "                \n",
    "    sorted_dict = {k: v for k, v in sorted(probabilities.items(), key=lambda x: x[1],reverse=True)}\n",
    "    probabilities_list = list(sorted_dict.keys())\n",
    "      \n",
    "    # Clear the existing text in the correction label widget\n",
    "    suggested_candidates.delete(0, \"end\")\n",
    "    \n",
    "    for candidate in probabilities_list:\n",
    "        suggested_candidates.insert(\"end\", candidate + \"\\n\")\n",
    "        \n",
    "def replace_word_with_candidate(event):\n",
    "    global clicked_word\n",
    "    \n",
    "    candidate_index = suggested_candidates.curselection()\n",
    "    \n",
    "    if candidate_index:\n",
    "        clicked_candidate = suggested_candidates.get(candidate_index)\n",
    "        clicked_candidate = clicked_candidate.strip()\n",
    "        \n",
    "        sentence = editor.get(\"1.0\", \"end\")\n",
    "        \n",
    "        updated_text = sentence.replace(clicked_word, clicked_candidate)\n",
    "    \n",
    "        # Clear the editor and insert the updated text\n",
    "        editor.delete(\"1.0\", \"end\")\n",
    "        editor.insert(\"end\", updated_text)\n",
    "        \n",
    "def dictionary_search():\n",
    "    search_word = search_text.get(1.0, \"end\").strip().lower()\n",
    "    \n",
    "    if search_word:\n",
    "        found = False\n",
    "        dictionary_list.selection_clear(0, 'end')\n",
    "        for index in range(dictionary_list.size()):\n",
    "            word = dictionary_list.get(index)\n",
    "            if search_word == word.lower().strip():\n",
    "                dictionary_list.selection_set(index)\n",
    "                dictionary_list.see(index)\n",
    "                dictionary_list.activate(index)\n",
    "                dictionary_list.itemconfig(index, bg = 'yellow')\n",
    "                found = True\n",
    "            else:\n",
    "                dictionary_list.itemconfig(index, bg = 'white')\n",
    "                                  \n",
    "wn = Tk()\n",
    "wn.geometry(\"1000x1000\")\n",
    "wn.configure(bg='azure2')\n",
    "wn.title(\"Spell Checker\")\n",
    "\n",
    "searchWord = StringVar()\n",
    "\n",
    "headingFrame1 = Frame(wn, bg=\"gray91\", bd=5)\n",
    "headingFrame1.place(relx=0.05, rely=0.01, relwidth=0.9, relheight=0.1)\n",
    "\n",
    "headingLabel = Label(headingFrame1, text=\"Automatic Spelling Correction System\", fg='grey19', font=('Courier', 20, 'bold'))\n",
    "headingLabel.place(relx=0, rely=0, relwidth=1, relheight=1)\n",
    "\n",
    "# Legend\n",
    "Label(wn, text='Legend: Red colour = Non-word error', bg='azure2', font=('Italian', 12)).place(x=20, y=130)\n",
    "Label(wn, text='Yellow colour = Real-word error', bg='azure2', font=('Italian', 12)).place(x=82, y=170)\n",
    "\n",
    "# Editor section\n",
    "Label(wn, text='Please input sentence', bg='azure2', font=('Courier', 12)).place(x=20, y=220)\n",
    "\n",
    "editor = Text(wn, height=12, width=106, font=('calibre', 12, 'normal'))\n",
    "editor.place(x=20, y=250)\n",
    "\n",
    "# Configure a tag for non-word and real word errors\n",
    "editor.tag_configure(\"non-word error\", background=\"red\")\n",
    "editor.tag_configure(\"real word error\", background = \"yellow\")\n",
    "\n",
    "# Bind the function to the wrong word click event\n",
    "editor.tag_bind(\"non-word error\", \"<Button-1>\", generate_word_candidates)\n",
    "editor.tag_bind(\"real word error\", \"<Button-1>\", generate_real_word_candidates)\n",
    "\n",
    "# Dictionary List section\n",
    "dictionary_label = Label(wn, text='Dictionary List:', bg='azure2', font=('Courier', 12))\n",
    "dictionary_label.place(x=20, y=480)\n",
    "\n",
    "dictionary_list = Listbox(wn, height=7, width=50, font=('calibre', 12, 'normal'))\n",
    "dictionary_list.place(x=20, y=500)\n",
    "\n",
    "# Initialize the dictionary list when run GUI \n",
    "initialize_dictionary_list()\n",
    "\n",
    "# Search Section\n",
    "search_label = Label(wn, text = 'Search:', bg = 'azure2', font = ('Courier', 12))\n",
    "search_label.place(x = 20, y = 650)\n",
    "\n",
    "search_text = Text(wn, height = 2, width = 20, font = ('calibre', 12, 'normal'))\n",
    "search_text.place(x = 20, y = 675)\n",
    "\n",
    "# Search Button\n",
    "search_button = Button(wn, text = 'Search', bg='honeydew2', fg='black', width=15, height=1, command = dictionary_search)\n",
    "search_button['font'] = font.Font(size = 12)\n",
    "search_button.place(x = 20, y = 725)\n",
    "\n",
    "# Check button\n",
    "check_button = Button(wn, text='Check', bg='honeydew2', fg='black', width=15, height=1, \n",
    "                      command=combine_funcs(detect_non_word_errors, detect_real_word_errors))\n",
    "check_button['font'] = font.Font(size=14)\n",
    "check_button.place(x=780, y=525)\n",
    "\n",
    "# Section to display corrected words\n",
    "suggested_candidates_label = Label(wn, text='Candidate Words:', bg='azure2', font=('Courier', 12))\n",
    "suggested_candidates_label.place(x=500, y=480)\n",
    "\n",
    "suggested_candidates = Listbox(wn, height=7, width=25, font=('calibre', 12, 'normal'))\n",
    "suggested_candidates.place(x=500, y=500)\n",
    "suggested_candidates.bind(\"<<ListboxSelect>>\", replace_word_with_candidate)\n",
    "\n",
    "wn.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14d18a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
